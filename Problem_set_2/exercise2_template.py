from __future__ import print_function, division
import os
import scipy.ndimage
import torch
import random
import matplotlib.pyplot as plt
import numpy as np
from torch.utils.data import DataLoader
from torchvision import transforms
import torch.nn as nn
import torch.optim as optim


# different type of galaxies
classes = ['spiral', 'elliptical', 'uncertain']

# detect CUDA
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print("Using device", device)


def get_class(one_hot_vec):
    global classes
    for i in range(len(one_hot_vec)):
        if one_hot_vec[i] == 1:
            return classes[i]


########################################################################################################################
# (a) read in array with information as generated by get_images.py and look at a few examples
########################################################################################################################
hnd = open("./GalaxyZoo/training_data_2.txt", "r")
all_data = eval(hnd.read())
hnd.close()

# look at a few images
counter = 0
for entry in all_data:
    image = plt.imread(os.path.join('./GalaxyZoo/', entry[0] + '.jpg'))
    plt.figure()
    plt.title(get_class(entry[1:]))
    plt.imshow(image)
    plt.show()
    plt.imshow(scipy.ndimage.rotate(image, 90))
    plt.show()

    counter += 1
    if counter == 3:
        break


########################################################################################################################
# (b) normalize to have zero mean and unit variance
########################################################################################################################
# implement the transformation. Torch provides this functionality in torchvision.transforms


########################################################################################################################
# (c), (d) over-/under-sample for a balanced training set and perform a train : test split of 90 : 10
########################################################################################################################
max_num = 5000
max_num = min(max_num, len(all_data))
split_point = int(np.floor(0.9*max_num))

train_set, test_set = [], []
# implement split and over-/under-sampling here
print("(# train, # test): (" + str(len(train_set)) + ", " + str(len(test_set)) + ")\n")

# look at distribution of three classes in train set
distro = np.array([0 for _ in range(len(classes))])
for e in train_set:
    distro = np.add(distro, e[1].numpy())

for i in range(len(classes)):
    print("{:12s}: {:1.2f}".format(classes[i], distro[i]/float(len(train_set))))


########################################################################################################################
# (e) implement the NN
########################################################################################################################

# helper function to compute the output dimension after a convolution
def get_conv_out_dim(size, padding, dilation, kernel, stride):
    return int(np.floor((size+2*padding-dilation*(kernel-1)-1)/stride + 1))


w0 = 120
w1 = get_conv_out_dim(w0, 0, 1, 8, 2)
w2 = get_conv_out_dim(w1, 0, 1, 2, 2)
w3 = get_conv_out_dim(w2, 0, 1, 8, 2)
w4 = get_conv_out_dim(w3, 0, 1, 2, 2)


class Classifyer_CNN(nn.Module):
    def __init__(self):
        super(Classifyer_CNN, self).__init__()
        self.conv1 = nn.Conv2d(3, 6, 8, (2, 2))  # in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, bias, padding_mode
        self.act1 = nn.LeakyReLU()
        # define the rest of the layers/activation functions here

    def forward(self, x):
        x = self.act1(self.conv1(x))
        # define the rest of the NN here
        return x


net = Classifyer_CNN()
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)
net.to(device)


########################################################################################################################
# (f) Train the NN and evaluate the performance
########################################################################################################################
trainloader = DataLoader(dataset=train_set, batch_size=1, shuffle=True)
testloader = DataLoader(dataset=test_set, batch_size=1, shuffle=False)
num_epochs = 10
for epoch in range(num_epochs):  # loop over the dataset multiple times
    # implement training loop here, as in Problem Set 1.

print('Finished Training')
